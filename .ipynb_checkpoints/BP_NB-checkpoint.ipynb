{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "GR5YlMZFZWM2"
   },
   "outputs": [],
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DDiePZRrZdvJ"
   },
   "source": [
    "**Install required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-g-lrETKWWHz",
    "outputId": "c4e3e38d-85b7-4738-9a9b-57d4cad3899a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n",
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement os (from versions: )\n",
      "No matching distribution found for os\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q webhoseio\n",
    "!pip install -q sklearn\n",
    "!pip install -q os\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xkLUWNzIAYI"
   },
   "source": [
    "**Import required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FapJyEuJH07k",
    "outputId": "c3e0ef23-792b-40c1-8af3-35f372290c25"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import webhoseio\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4mhr0WcIDLT"
   },
   "source": [
    "**Define global variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nJ5IFpjWExBH",
    "outputId": "53742641-3bc6-4e1e-cc96-a43c1eaf3395"
   },
   "outputs": [],
   "source": [
    "articles = []\n",
    "types = list()\n",
    "words = list()\n",
    "#NLTKÂ´S stopwords + mine\n",
    "stopWords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\"ers\", \"yourself\", \"yourselves\", \"he\",\"isnt\",\"cant\" \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"wasnt\", \"were\", \"be\", \"been\", \"being\", \"have\", \"havent\", \"has\", \"had\", \"having\", \"do\", \"does\", \"doesnt\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "trainingPercentage = 80\n",
    "amountOfArticlesLoad = 2000\n",
    "\n",
    "class Regs: \n",
    "    specialChars = '' \n",
    "    digits = '' \n",
    "    singleChars = ''\n",
    "    multipleWhiteSpaces = ''\n",
    "    stopWords = list()\n",
    "regexes = Regs()\n",
    "regexes.specialChars = re.compile('[^\\w\\s]')\n",
    "regexes.digits = re.compile('\\d')\n",
    "regexes.singleChars = re.compile('\\s.\\s')\n",
    "regexes.multipleWhiteSpaces = re.compile('[ ]{2,}')\n",
    "for sw in stopWords:\n",
    "    exp = '\\\\b' + sw + '+\\W'\n",
    "    regexes.stopWords.append(re.compile(exp))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55m5S5iQZigX"
   },
   "source": [
    "**Download and create files for the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "QetcdiOaVVEo",
    "outputId": "d0fe2ea0-3a80-4cf5-ba26-38524c53dc27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data for sports category, and created sportsdata.json file\n",
      "Downloaded data for sports category, and created businessdata.json file\n"
     ]
    }
   ],
   "source": [
    "#decide if the data is needed to download again\n",
    "#do not load on every run, becouse of request limit\n",
    "#Config the API connection with the token\n",
    "webhoseio.config(token=\"d0d26a65-a56f-413d-acc8-e54bafbb7377\") \n",
    "#Create queryparams for sports data\n",
    "query_params_sport = {\n",
    "    \"q\": \"site_category:sports language:english site_type:news domain_rank:<100\",\n",
    "    \"sort\": \"relevancy\"\n",
    "    }\n",
    "#query articles for sport\n",
    "output = webhoseio.query(\"filterWebContent\", query_params_sport)\n",
    "#save data to file\n",
    "with open('sportsdata.json', 'w') as outfile:\n",
    "    json.dump(output, outfile)\n",
    "print(\"Downloaded data for sports category, and created sportsdata.json file\")\n",
    "\n",
    "#Create queryparams for business data\n",
    "query_params_business = {\n",
    "    \"q\": \"site_category:business language:english site_type:news domain_rank:<100\",\n",
    "    \"sort\": \"relevancy\"\n",
    "    }\n",
    "#query articles for sport\n",
    "output = webhoseio.query(\"filterWebContent\", query_params_business)\n",
    "#save data to file\n",
    "with open('businessdata.json', 'w') as outfile:\n",
    "    json.dump(output, outfile)\n",
    "print(\"Downloaded data for sports category, and created businessdata.json file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ll__CPZRDMfz"
   },
   "source": [
    "**Data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rp_26QzoDMQZ",
    "outputId": "cd1a05c9-8d39-4506-b7c5-c7c9617aaacd"
   },
   "outputs": [],
   "source": [
    "def preprocessText(text):\n",
    "    text = text.lower()\n",
    "    #new lines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    #special characters\n",
    "    text = re.sub(regexes.specialChars, '', text)\n",
    "    #digits\n",
    "    text = re.sub(regexes.digits, '', text)\n",
    "    #stopwords\n",
    "    for sw in regexes.stopWords:\n",
    "        text = re.sub(sw , '', text)\n",
    "    #single characters (ex donald j trump => donald trump)\n",
    "    text = re.sub(regexes.singleChars, ' ', text)\n",
    "    #multiple white spaces\n",
    "    text = re.sub(regexes.multipleWhiteSpaces, ' ', text)\n",
    "    return(text)\n",
    "print(\"fucntion preprocessText done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UBAgXm2rGTA_"
   },
   "source": [
    "**Processing data**\n",
    "\n",
    "\n",
    "1.   Get data from files \n",
    "\n",
    "2.   Iterate trough the texts and preprocess them\n",
    "\n",
    "3.   Create the dictionary from all texts and then remove duplicates\n",
    "\n",
    "4.   Create the matrix which will contain 0/1 showing if the word is in the text. First column represents the label, all other columns represents the words in dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "id": "be93Kj2VeCJL",
    "outputId": "1cb2f563-2150-47f8-b0d7-48160d579911"
   },
   "source": [
    "##### #load data for sports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sportArticles = listdir('DATA/Sports')\n",
    "if len(sportArticles) > 0 :\n",
    "    print(\"Preprocessing of \" + str(amountOfArticlesLoad*trainingPercentage / 100) + \" sports articles starting\")\n",
    "    start = time.time()\n",
    "    for index, article in enumerate(sportArticles, start=0):\n",
    "        if index < (amountOfArticlesLoad*trainingPercentage / 100) :\n",
    "            with open(\"DATA/Sports/\" + article, encoding=\"utf8\") as json_data:\n",
    "                text = json.load(json_data)[\"text\"]\n",
    "                text = preprocessText(text)\n",
    "                articles.append(text.split(' '))\n",
    "                types.append(\"1\")     \n",
    "    end = time.time()\n",
    "    print(\"Preprocessing sports data finished in \",end - start)\n",
    "#load data for business\n",
    "financeArticles = listdir(\"DATA/Finance\")\n",
    "if len(financeArticles) > 0 :\n",
    "    print(\"Preprocessing of \" + str(amountOfArticlesLoad*trainingPercentage / 100) + \" business articles starting\")\n",
    "    start = time.time()\n",
    "    for index, article in enumerate(financeArticles, start=0):\n",
    "        if index < (amountOfArticlesLoad*trainingPercentage / 100) :\n",
    "            with open(\"DATA/Finance/\" + article, encoding=\"utf8\") as json_data:\n",
    "                text = json.load(json_data)[\"text\"]\n",
    "                text = preprocessText(text)\n",
    "                articles.append(text.split(' '))\n",
    "                types.append(\"0\")    \n",
    "               \n",
    "    end = time.time()\n",
    "    print(\"Preprocessing finished in \",end - start)\n",
    "start = time.time()\n",
    "#create one array fromm all articles\n",
    "words = [item for sublist in articles for item in sublist]\n",
    "#remove duplicate values from words list\n",
    "words = list(set(words))\n",
    "words.remove(\"\")\n",
    "words = sorted(words)\n",
    "wordsDictionary = dict((v, i) for i, v in enumerate(words))\n",
    "end = time.time()\n",
    "print(\"Creating dictionary finished in \",end - start)\n",
    "print(\"There are \" + str(len(words)) + \" words\")\n",
    "\n",
    "print(\"Creating matrix of containing words\")\n",
    "start = time.time()\n",
    "articleWords = np.zeros((len(articles), len(words) + 1))\n",
    "for index, article in enumerate(articles, start=0):\n",
    "    articleWords[index, 0 ] = types[index] \n",
    "    for j, word in enumerate(article, start=0):\n",
    "        if word != '':\n",
    "            articleWords[index, wordsDictionary[word] + 1] = 1        \n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Creating matrix finished in \",end - start)\n",
    "np.savetxt(\"foo.csv\", articleWords, fmt='%1.0f', delimiter=\" \")\n",
    "    \n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Um4ihLmHeIsh",
    "outputId": "3ddee964-a8c5-4d58-d946-c763e8508c4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n",
      "6400\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsDictionary))\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xLvGiZ7EI3FG"
   },
   "source": [
    "**Creating the matrix containg the probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VY3MUxmaI6UW",
    "outputId": "1f53d431-6d67-455a-80fa-9ac5f189fe7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the matrix containg the probabilities finished in  10.663165092468262\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting creation of matrix\")\n",
    "start = time.time()\n",
    "X = articleWords\n",
    "#calculate the phi values\n",
    "Y = np.zeros((2, X.shape[1]))\n",
    "#number of rows (number of samples in training set)\n",
    "m = X.shape[0]\n",
    "\n",
    "#number of words\n",
    "n = X.shape[1] - 1\n",
    "\n",
    "#number of rows clasified to first / second group\n",
    "sumY1 = float(np.sum(X[:,0]))\n",
    "sumY0 = float(m - np.sum(X[:,0]))\n",
    "\n",
    "#calcualting Phi_{y=0} and Phi_{y=1}\n",
    "Y[0,0] = (sumY0 + 1) / float(m + 2)\n",
    "Y[1,0] = (sumY1 + 1) / float(m + 2)\n",
    "\n",
    "for j in range(1, X.shape[1]):\n",
    "    #calcualting Phi_{j|y=0} and Phi_{j|y=1}\n",
    "    Y[0,j] = (np.sum(X[X[:,0]==1,j]) + 1) / (sumY0 + 2)\n",
    "    Y[1,j] = (np.sum(X[X[:,0]==0,j]) + 1) / (sumY1 + 2)\n",
    "      \n",
    "end = time.time()\n",
    "print(\"Creating the matrix containg the probabilities finished in \",end - start)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "KbEu4gCogEoa",
    "outputId": "4aebf9d7-a159-4645-99e4-b61ddeff0184"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(Y[0,1:], bins=[0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0tL6-TII_vu"
   },
   "source": [
    "**Test the algoritm on sports test articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "qprIT_vzI_QI",
    "outputId": "35fc749c-3fd0-43ed-b964-cb3117488717"
   },
   "outputs": [],
   "source": [
    "print(\"Startin the test on finance data\")\n",
    "businessSucces = 0\n",
    "for index, article in enumerate(financeArticles, start=0):\n",
    "    if index >= (amountOfArticlesLoad*trainingPercentage / 100 ) and index <= amountOfArticlesLoad:\n",
    "        with open(\"DATA/Finance/\" + article, encoding=\"utf8\") as json_data:\n",
    "            testtext = json.load(json_data)[\"text\"]\n",
    "            testtext = preprocessText(testtext)\n",
    "            testarticle = testtext.split(' ')\n",
    "            testArticleWords = np.zeros((1, len(words)))\n",
    "            for k, word in enumerate(testarticle, start=0):\n",
    "                if word != '' :\n",
    "                    if word in words:\n",
    "                        testArticleWords[0, words.index(word)] = 1\n",
    "\n",
    "            a = np.log(Y[1,0])\n",
    "            b = np.log(Y[0,0])\n",
    "\n",
    "            #get probability that this vector is classified to first gorup y=0\n",
    "            for j in range(1, X.shape[1]):\n",
    "                if(testArticleWords[0,j - 1] == 0):\n",
    "                    a = a + np.log((1 - Y[1,j]))\n",
    "                    b = b + np.log((1 - Y[0,j]))\n",
    "                else:\n",
    "                    a = a + np.log(Y[1,j])\n",
    "                    b = b + np.log(Y[0,j])\n",
    "\n",
    "            phi0z = 1 / ( 1 + np.exp(a - b))\n",
    "            phi1z = 1 / ( 1 + np.exp(b - a))\n",
    "\n",
    "            if(phi0z > 0.5):\n",
    "                businessSucces = businessSucces + 1\n",
    "\n",
    "            #print(\"Text belongs to group 0:\")\n",
    "            #print(phi0z)\n",
    "\n",
    "            #print(\"Text belongs to group 1:\")\n",
    "            #print(phi1z)\n",
    "print(\"businessucces\")\n",
    "print(businessSucces)\n",
    "print(businessSucces / (amountOfArticlesLoad*(100 - trainingPercentage) / 100) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text belongs to group 0:\n",
      "0.5833312342487992\n",
      "Text belongs to group 1:\n",
      "0.4166687657512009\n"
     ]
    }
   ],
   "source": [
    "testtext = businessData[\"posts\"][89][\"text\"]\n",
    "testtext = preprocessText(testtext)\n",
    "testarticle = testtext.split(' ')\n",
    "testArticleWords = np.zeros((1, len(words)))\n",
    "for k, word in enumerate(testarticle, start=0):\n",
    "    if word != '' :\n",
    "        if word in words:\n",
    "            testArticleWords[0, words.index(word)] = 1\n",
    "\n",
    "a = np.log(Y[1,0])\n",
    "b = np.log(Y[0,0])\n",
    "\n",
    "#get probability that this vector is classified to first gorup y=0\n",
    "for j in range(1, X.shape[1]):\n",
    "    if(testArticleWords[0,j - 1] == 0):\n",
    "        a = a + np.log((1 - Y[1,j]))\n",
    "        b = b + np.log((1 - Y[0,j]))\n",
    "    else:\n",
    "        a = a + np.log(Y[1,j])\n",
    "        b = b + np.log(Y[0,j])\n",
    "\n",
    "phi0z = 1 / ( 1 + np.exp(a - b))\n",
    "phi1z = 1 / ( 1 + np.exp(b - a))\n",
    "\n",
    "\n",
    "print(\"Text belongs to group 0:\")\n",
    "print(phi0z)\n",
    "\n",
    "print(\"Text belongs to group 1:\")\n",
    "print(phi1z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DU2GcaOGuOcG"
   },
   "source": [
    "**SKLEARN - reproduce our algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "t1tzXhrQxsu7",
    "outputId": "e9300070-8500-45c9-c66c-8f23c876df90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11427)\n",
      "(170, 11428)\n",
      "(2, 11428)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(testArticleWords.shape)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "print(YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pyUNYlwdupxx",
    "outputId": "bccc3850-c9f6-4112-9781-3c468324d8bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "businessucces\n",
      "[3.]\n",
      "[50.]\n"
     ]
    }
   ],
   "source": [
    "XX = articleWords[:,1:]\n",
    "YY = articleWords[:,0]\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(XX, YY)\n",
    "\n",
    "businessSucces = 0\n",
    "for index, article in enumerate(businessData[\"posts\"], start=0):\n",
    "    if index >= (len(businessData[\"posts\"])*trainingPercentage / 100):\n",
    "        testtext = businessData[\"posts\"][index][\"text\"]\n",
    "        testtext = preprocessText(testtext)\n",
    "        testarticle = testtext.split(' ')\n",
    "        testArticleWords = np.zeros((1, len(words)))\n",
    "        for k, word in enumerate(testarticle, start=0):\n",
    "            if word != '' :\n",
    "                if word in words:\n",
    "                    testArticleWords[0, words.index(word)] = 1\n",
    "\n",
    "        businessSucces = businessSucces + clf.predict(testArticleWords)\n",
    "        \n",
    "print(\"businessucces\")\n",
    "print(businessSucces)\n",
    "print(businessSucces / (len(businessData[\"posts\"])*(100 - trainingPercentage) / 100) * 100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BP-NB.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
