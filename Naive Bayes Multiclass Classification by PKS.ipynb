{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "2018-11-24 09:32\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "from nltk.stem import PorterStemmer\n",
    "from os import listdir\n",
    "\n",
    "class Regs: \n",
    "    specialChars = '' \n",
    "    digits = '' \n",
    "    singleChars = ''\n",
    "    multipleWhiteSpaces = ''\n",
    "    stopWords = list()\n",
    "stopWords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\"ers\", \"yourself\", \"yourselves\", \"he\",\"isnt\",\"cant\" \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"wasnt\", \"were\", \"be\", \"been\", \"being\", \"have\", \"havent\", \"has\", \"had\", \"having\", \"do\", \"does\", \"doesnt\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Precompile regexes for better performance\n",
    "def compileRegexes():\n",
    "    regexes = Regs()\n",
    "    regexes.specialChars = re.compile('[^\\w\\s]')\n",
    "    regexes.digits = re.compile('\\d')\n",
    "    regexes.singleChars = re.compile('\\s.\\s')\n",
    "    regexes.multipleWhiteSpaces = re.compile('[ ]{2,}')\n",
    "    for sw in stopWords:\n",
    "        exp = '\\\\b' + sw + '+\\W'\n",
    "        regexes.stopWords.append(re.compile(exp))\n",
    "    return(regexes)\n",
    "\n",
    "#Format the index into string with length 7\n",
    "def formatIndex(index):\n",
    "    i = str(index)\n",
    "    while len(i) < 7:\n",
    "        i = \"0\" + i\n",
    "    return(i)\n",
    "\n",
    "#Gather the texts from the source, based on starting index, amount of article and class\n",
    "#=> returns array of texts and array of classes\n",
    "def gatherTexts(startingIndex, amountOfArticles, includeClasses):\n",
    "    if len(includeClasses) < 1:\n",
    "        print(\"There must be at least 1 class. eg: [0,1,2]\")\n",
    "    \n",
    "    #Create two arrays, which will be returned from this function\n",
    "    texts = []\n",
    "    types = []\n",
    "    \n",
    "    #Get data for class 0 => in our project it is FINANCE\n",
    "    if 0 in includeClasses:\n",
    "        articles = listdir(\"DATA/Finance\")\n",
    "        nArticles = len(articles)\n",
    "        #check if there are enough articles for the input paramteres\n",
    "        if (nArticles > startingIndex + amountOfArticles) :\n",
    "            for ind in range(startingIndex, startingIndex + amountOfArticles):\n",
    "                with open(\"DATA/Finance/news_\" + formatIndex(ind) + \".json\", encoding=\"utf8\") as json_data:\n",
    "                    texts.append(json.load(json_data)[\"text\"])\n",
    "                    types.append(\"0\")   \n",
    "\n",
    "    #Get data for class 1 => in our project it is SPORT\n",
    "    if 1 in includeClasses:\n",
    "        articles = listdir(\"DATA/Sports\")\n",
    "        nArticles = len(articles)\n",
    "        #check if there are enough articles for the input paramteres\n",
    "        if (nArticles > startingIndex + amountOfArticles) :\n",
    "            for ind in range(startingIndex, startingIndex + amountOfArticles):\n",
    "                with open(\"DATA/Sports/news_\" + formatIndex(ind) + \".json\", encoding=\"utf8\") as json_data:\n",
    "                    texts.append(json.load(json_data)[\"text\"])\n",
    "                    types.append(\"1\")   \n",
    "                    \n",
    "    #Get data for class 2 => in our project it is TECHNOLOGY\n",
    "    if 2 in includeClasses:\n",
    "        articles = listdir(\"DATA/Technology\")\n",
    "        nArticles = len(articles)\n",
    "        #check if there are enough articles for the input paramteres\n",
    "        if (nArticles > startingIndex + amountOfArticles) :\n",
    "            for ind in range(startingIndex, startingIndex + amountOfArticles):\n",
    "                with open(\"DATA/Technology/news_\" + formatIndex(ind) + \".json\", encoding=\"utf8\") as json_data:\n",
    "                    texts.append(json.load(json_data)[\"text\"])\n",
    "                    types.append(\"2\")   \n",
    "                    \n",
    "    #Get data for class 3 => in our project it is ENTERTAINMENT\n",
    "    if 3 in includeClasses:\n",
    "        articles = listdir(\"DATA/Entertainment\")\n",
    "        nArticles = len(articles)\n",
    "        #check if there are enough articles for the input paramteres\n",
    "        if (nArticles > startingIndex + amountOfArticles) :\n",
    "            for ind in range(startingIndex, startingIndex + amountOfArticles):\n",
    "                with open(\"DATA/Entertainment/news_\" + formatIndex(ind) + \".json\", encoding=\"utf8\") as json_data:\n",
    "                    texts.append(json.load(json_data)[\"text\"])\n",
    "                    types.append(\"3\") \n",
    "                    \n",
    "    return {'texts':texts, 'types':types}\n",
    "\n",
    "def preprocessTexts(texts,types, regexes, usePorterStemmer = 0, removeUnfrequent = 0, removeFrequent = 0):    \n",
    "    articles = []\n",
    "    for index, text in enumerate(texts, start=0):\n",
    "        articles.append(preprocessArticle(text,regexes,usePorterStemmer))\n",
    "        \n",
    "    #create one array fromm all articles\n",
    "    words = []\n",
    "    words = [item for sublist in articles for item in sublist]\n",
    "    #remove duplicate values from words list\n",
    "    words = list(set(words))\n",
    "    words = sorted(words)\n",
    "    wordsDictionary = dict((v, i) for i, v in enumerate(words))\n",
    "    \n",
    "    articleWords = np.zeros((len(articles), len(words) + 1))\n",
    "    for index, article in enumerate(articles, start=0):\n",
    "        articleWords[index, 0 ] = types[index] \n",
    "        for j, word in enumerate(article, start=0):\n",
    "            if word != '':\n",
    "                articleWords[index, wordsDictionary[word] + 1] = 1   \n",
    "                \n",
    "    #Remove words with occurance = removeUnfrequent\n",
    "    if removeUnfrequent >= 1:    \n",
    "        indexes = []\n",
    "        for word in wordsDictionary:\n",
    "            if(sum(articleWords[:,wordsDictionary[word]]) > removeUnfrequent):\n",
    "                indexes.append(wordsDictionary[word])\n",
    "        ind = np.array(indexes)\n",
    "        words = np.array(words)\n",
    "        words = words[ind].tolist()\n",
    "        articleWords = articleWords[:,np.insert(ind + 1, 0,0, axis=0)]\n",
    "    \n",
    "    #Remove words, which are the most frequent => TOP removeFrequent will be removed\n",
    "    if removeFrequent > 0:\n",
    "        usage = np.zeros(len(words)+1)\n",
    "        for i in range(1, len(words)+1):\n",
    "            usage[i] = sum(articleWords[:,i])\n",
    "        for i in range(0, removeFrequent):\n",
    "            words = np.delete(words,usage.tolist().index(max(usage)),0)\n",
    "            articleWords = np.delete(articleWords, usage.tolist().index(max(usage)),1)\n",
    "            usage = np.delete(usage,usage.tolist().index(max(usage)),0)\n",
    "            \n",
    "    return{'articleWords':articleWords, 'words':words}\n",
    "\n",
    "\n",
    "#Create matrix containg all Phi values\n",
    "#returns array(nClasses x amountOfWords)\n",
    "def createPhiMatrix(articleWords, includeClasses):\n",
    "    numberOfClasses = len(includeClasses)\n",
    "    Y = np.zeros((numberOfClasses, articleWords.shape[1]))\n",
    "    \n",
    "    sumClasses = np.zeros((numberOfClasses))\n",
    "    #number of rows clasified to first / second group\n",
    "    for c, index in enumerate(includeClasses, start = 0):\n",
    "        sumClasses[index] = float(len(articleWords[articleWords[:,0] == c,0]))\n",
    "\n",
    "    #Laplace => alfa = 1\n",
    "    alfa = 1\n",
    "\n",
    "    #calcualting Phi_{y=0} and Phi_{y=1}\n",
    "    for j in range(0, numberOfClasses):\n",
    "        Y[j,0] = (sumClasses[j] + alfa) / float(articleWords.shape[0] + numberOfClasses * alfa)\n",
    "\n",
    "    for j in range(1, articleWords.shape[1]):\n",
    "        #calcualting Phi_{j|y=0} and Phi_{j|y=1}\n",
    "        for k in range(0, numberOfClasses):\n",
    "            Y[k,j] = (np.sum(articleWords[articleWords[:,0]==includeClasses[k],j]) + alfa) / (sumClasses[k] + numberOfClasses * alfa)\n",
    "    \n",
    "    return(Y)\n",
    "    \n",
    "#Preprocess the article with the precompiled regexes, and optional stemmer \n",
    "#=> returns the text splitted into array of words\n",
    "def preprocessArticle(text, regexes, usePorterStemmer):\n",
    "    text = text.lower()\n",
    "    #new lines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('_', ' ')\n",
    "    #special characters\n",
    "    text = re.sub(regexes.specialChars, ' ', text)\n",
    "    #digits\n",
    "    text = re.sub(regexes.digits, '', text)\n",
    "    #stopwords\n",
    "    for sw in regexes.stopWords:\n",
    "        text = re.sub(sw , '', text)\n",
    "    #single characters (ex donald j trump => donald trump)\n",
    "    text = re.sub(regexes.singleChars, ' ', text)\n",
    "    #multiple white spaces\n",
    "    text = re.sub(regexes.multipleWhiteSpaces, ' ', text)\n",
    "    if usePorterStemmer == 1:\n",
    "        splitted = text.split()\n",
    "        for index, word in enumerate(splitted, start=0):\n",
    "            splitted[index] = ps.stem(word)\n",
    "        return splitted\n",
    "    return(text.split())\n",
    "\n",
    "\n",
    "#Make prediction based on \n",
    "def predict(phiMatrix, text,words, regexes, usePorterStemmer):\n",
    "    nClasses = phiMatrix.shape[0]\n",
    "    Y = phiMatrix\n",
    "    textPorcessed = preprocessArticle(text,regexes,usePorterStemmer)\n",
    "    testArticleWords = np.zeros((1, len(words)))\n",
    "    for k, word in enumerate(textPorcessed, start=0):\n",
    "            if word in words:\n",
    "                testArticleWords[0, words.index(word)] = 1\n",
    "\n",
    "    P = np.zeros(nClasses)\n",
    "    for j in range(0, nClasses):\n",
    "        P[j] = np.log(Y[j,0]) + ((np.log(np.array(Y[j,1:]*testArticleWords)[testArticleWords == 1]))).sum() + ((np.log(np.array((1 - Y[j,1:])*(testArticleWords - 1)*-1)[testArticleWords == 0]))).sum()\n",
    "\n",
    "    classPossibilities = np.zeros(4)\n",
    "    for i in range(0,phiMatrix.shape[0]):\n",
    "        classPossibilities[i] = 1 / (sum(np.exp(P-P[i])))\n",
    "    \n",
    "    return(np.argmax(classPossibilities))\n",
    "\n",
    "print(\"Done\")\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.015621185302734375\n"
     ]
    }
   ],
   "source": [
    "rawTexts =  gatherTexts(1,100,np.array([0,1,2,3]))\n",
    "regs = compileRegexes()\n",
    "preprocessedTexts = preprocessTexts(rawTexts[\"texts\"],rawTexts[\"types\"],regs)\n",
    "phi = createPhiMatrix(preprocessedTexts[\"articleWords\"],np.array([0,1,2,3]))\n",
    "with open(\"DATA/Sport/news_0002100.json\", encoding=\"utf8\") as json_data:\n",
    "    #print(index)\n",
    "    testtext = json.load(json_data)[\"text\"]\n",
    "    start = time.time()\n",
    "    print(predict(phi,testtext,preprocessedTexts[\"words\"],regs,0))\n",
    "    end = time.time()\n",
    "    print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "        0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "        1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
       "        1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "        0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessedTexts[\"articleWords\"][:,np.insert(np.array(np.sum(preprocessedTexts[\"articleWords\"][preprocessedTexts[\"articleWords\"][:,0]==0,1:], axis=0) != 0),0, False)][preprocessedTexts[\"articleWords\"][:,0]==0,:]\n",
    "\n",
    "\n",
    "#print(len(preprocessedTexts[\"articleWords\"][1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1230\n"
     ]
    }
   ],
   "source": [
    "#print(preprocessedTexts[\"articleWords\"])\n",
    "rawTexts =  gatherTexts(1,50,np.array([0,1,2,3]))\n",
    "#regs = compileRegexes()\n",
    "preprocessedTexts = preprocessTexts(rawTexts[\"texts\"],rawTexts[\"types\"],regs)\n",
    "#for i in np.array([0,1]):\n",
    "    #print(np.mean(np.sum(preprocessedTexts[\"articleWords\"][:,np.insert(np.array(np.sum(preprocessedTexts[\"articleWords\"][preprocessedTexts[\"articleWords\"][:,0]==i,1:], axis=0) != 0),0, False)][preprocessedTexts[\"articleWords\"][:,0]==i,:], axis=0)))\n",
    "    #print(np.sum(np.sum(preprocessedTexts[\"articleWords\"][:,np.insert(np.array(np.sum(preprocessedTexts[\"articleWords\"][preprocessedTexts[\"articleWords\"][:,0]==i,1:], axis=0) != 0),0, False)][preprocessedTexts[\"articleWords\"][:,0]==i,:], axis=0) == 1))\n",
    "    #print(np.sum(np.sum(preprocessedTexts[\"articleWords\"][:,np.insert(np.array(np.sum(preprocessedTexts[\"articleWords\"][preprocessedTexts[\"articleWords\"][:,0]==i,1:], axis=0) != 0),0, False)][preprocessedTexts[\"articleWords\"][:,0]==i,:], axis=0) == 2))\n",
    "    #print(np.mean(np.sum(preprocessedTexts[\"articleWords\"][preprocessedTexts[\"articleWords\"][:,0]==i,1:], axis=1)))\n",
    "\n",
    "articleWords = preprocessedTexts[\"articleWords\"]\n",
    "#print(articleWords)\n",
    "\n",
    "wordsOfGrous0 = np.sum(articleWords[articleWords[:,0] == 0,1:], axis=0)\n",
    "wordsOfGrous1 = np.sum(articleWords[articleWords[:,0] == 1,1:], axis=0)\n",
    "\n",
    "print(np.array((np.where( np.logical_and( wordsOfGrous0 > 0, wordsOfGrous1 > 0 ) ))).size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a5b45285ab28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtesttext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtesttext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreprocessedTexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"words\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'phi' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"DATA/Finance/news_0002010.json\", encoding=\"utf8\") as json_data:\n",
    "    #print(index)\n",
    "    testtext = json.load(json_data)[\"text\"]\n",
    "    start = time.time()\n",
    "    print(predict(phi,testtext,preprocessedTexts[\"words\"],regs,0))\n",
    "    end = time.time()\n",
    "    print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5194274028629855\n",
      "98.56\n"
     ]
    }
   ],
   "source": [
    "#print(preprocessedTexts[\"articleWords\"])\n",
    "rawTexts =  gatherTexts(1,100,np.array([1]))\n",
    "regs = compileRegexes()\n",
    "preprocessedTexts = preprocessTexts(rawTexts[\"texts\"],rawTexts[\"types\"],regs)\n",
    "print(np.mean(np.sum(preprocessedTexts[\"articleWords\"][preprocessedTexts[\"articleWords\"][:,0]==1,1:], axis=0)))\n",
    "print(np.mean(np.sum(preprocessedTexts[\"articleWords\"][preprocessedTexts[\"articleWords\"][:,0]==1,1:], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPropertiesOfTrainingSet(startIndex, endIndex, usePorterStemmer = 0, removeUnfrequent = 0, removeFrequent = 0):\n",
    "    #STEPS:\n",
    "    #---1. Gather texts from all groups\n",
    "    #---2. Preprocess them\n",
    "    #---3. Create properties:\n",
    "    #------- Average number of words in the texts for each group\n",
    "    #------- Average texts where each word \n",
    "    \n",
    "    print(\"Starting the method for creating properties...\")\n",
    "    #Needed variables:    \n",
    "    regs = compileRegexes()\n",
    "    \n",
    "    #Get and preprocess texts \n",
    "    print(\"Starting the gathering of texts from \" + str(startIndex) + \" to \" + str(endIndex) + \"...\")\n",
    "    start = time.time()\n",
    "    rawTexts =  gatherTexts(startIndex,endIndex,np.array([0,1,2,3]))\n",
    "    end = time.time()\n",
    "    print(\"Gathering finished in \" + str(end-start) + \" ms\")\n",
    "    print(\"Starting the preprocess...\")\n",
    "    start = time.time()\n",
    "    preprocessedTexts = preprocessTexts(rawTexts[\"texts\"],rawTexts[\"types\"],regs, usePorterStemmer, removeUnfrequent, removeFrequent)\n",
    "    end = time.time()\n",
    "    print(\"Preprocessment finished in \" + str(end-start) + \" ms\")\n",
    "    \n",
    "    print(\"Starting the properties calculation for each group...\")\n",
    "    start = time.time()\n",
    "    averageAmountOfWords = np.zeros(4)\n",
    "    averageUsageOfWords = np.zeros(4)\n",
    "    amountOfRareWords1 = np.zeros(4)\n",
    "    amountOfRareWords2 = np.zeros(4)\n",
    "    totalAmountOfWords = np.zeros(4)\n",
    "    \n",
    "    articleWords = preprocessedTexts[\"articleWords\"]\n",
    "    \n",
    "    for i in np.array([0,1,2,3]):        \n",
    "        averageUsageOfWords[i] = np.mean(np.sum(articleWords[:,np.insert(np.array(np.sum(articleWords[articleWords[:,0]==i,1:], axis=0) != 0),0, False)][articleWords[:,0]==i,:], axis=0))\n",
    "        averageAmountOfWords[i] = np.mean(np.sum(articleWords[articleWords[:,0]==i,1:], axis=1))\n",
    "        amountOfRareWords1[i] = np.sum(np.sum(articleWords[:,np.insert(np.array(np.sum(articleWords[articleWords[:,0]==i,1:], axis=0) != 0),0, False)][articleWords[:,0]==i,:], axis=0) == 1)\n",
    "        amountOfRareWords2[i] = np.sum(np.sum(articleWords[:,np.insert(np.array(np.sum(articleWords[articleWords[:,0]==i,1:], axis=0) != 0),0, False)][articleWords[:,0]==i,:], axis=0) == 2)\n",
    "        totalAmountOfWords[i] = len(np.sum(articleWords[:,np.insert(np.array(np.sum(articleWords[articleWords[:,0]==i,1:], axis=0) != 0),0, False)][articleWords[:,0]==i,:], axis=0))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Properties for each group finished in \" + str(end-start) + \" ms\")\n",
    "    \n",
    "    print(\"Starting counting the common words trough the groups...\")\n",
    "    commonWords = np.zeros((4,4))\n",
    "    start = time.time()\n",
    "    for i in np.array([0,1,2,3]):\n",
    "        for j in np.array([0,1,2,3]): \n",
    "            wordsOfGrous0 = np.sum(articleWords[articleWords[:,0] == i,1:], axis=0)\n",
    "            wordsOfGrous1 = np.sum(articleWords[articleWords[:,0] == j,1:], axis=0)\n",
    "            commonWords[i,j] = np.array((np.where( np.logical_and( wordsOfGrous0 > 0, wordsOfGrous1 > 0 ) ))).size\n",
    "    end = time.time()\n",
    "    print(\"Common words counting finished in \" + str(end-start) + \" ms\")\n",
    "\n",
    "    return{'averageAmountOfWords':averageAmountOfWords, \n",
    "           'averageUsageOfWords':averageUsageOfWords,\n",
    "           'amountOfRareWords1':amountOfRareWords1,\n",
    "           'amountOfRareWords1Percentage':amountOfRareWords1 / totalAmountOfWords,\n",
    "           'amountOfRareWords2':amountOfRareWords2,\n",
    "           'amountOfRareWords2Percentage':amountOfRareWords2 / totalAmountOfWords,\n",
    "           'totalAmountOfWords':totalAmountOfWords,\n",
    "           'commonWords':commonWords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the method for creating properties...\n",
      "Starting the gathering of texts from 1 to 50...\n",
      "Gathering finished in 1.7856512069702148 ms\n",
      "Starting the preprocess...\n",
      "Preprocessment finished in 5.527027130126953 ms\n",
      "Starting the properties calculation for each group...\n",
      "Properties for each group finished in 0.2899503707885742 ms\n",
      "Starting counting the common words trough the groups...\n",
      "Common words counting finished in 0.16361737251281738 ms\n",
      "{'averageAmountOfWords': array([192.08, 120.78, 176.78, 141.96]), 'averageUsageOfWords': array([2.01341719, 2.17074047, 1.870292  , 1.80610687]), 'amountOfRareWords1': array([3070., 1804., 3197., 2716.]), 'amountOfRareWords1Percentage': array([0.64360587, 0.64845435, 0.67647059, 0.69109415]), 'amountOfRareWords2': array([739., 374., 712., 553.]), 'amountOfRareWords2Percentage': array([0.15492662, 0.13443566, 0.15065595, 0.14071247]), 'totalAmountOfWords': array([4770., 2782., 4726., 3930.]), 'commonWords': array([[4770., 1230., 2095., 1464.],\n",
      "       [1230., 2782., 1256., 1252.],\n",
      "       [2095., 1256., 4726., 1625.],\n",
      "       [1464., 1252., 1625., 3930.]])}\n"
     ]
    }
   ],
   "source": [
    "props = createPropertiesOfTrainingSet(1,50)\n",
    "print(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the method for creating properties...\n",
      "Starting the gathering of texts from 1 to 50...\n",
      "Gathering finished in 0.19225478172302246 ms\n",
      "Starting the preprocess...\n",
      "Preprocessment finished in 5.4446494579315186 ms\n",
      "Starting the properties calculation for each group...\n",
      "Properties for each group finished in 0.17907261848449707 ms\n",
      "Starting counting the common words trough the groups...\n",
      "Common words counting finished in 0.1216425895690918 ms\n",
      "{'averageAmountOfWords': array([175.06, 114.22, 162.86, 135.04]), 'averageUsageOfWords': array([2.52466109, 2.47873264, 2.31072645, 2.11528822]), 'amountOfRareWords1': array([2002., 1400., 2161., 2025.]), 'amountOfRareWords1Percentage': array([0.57744448, 0.60763889, 0.61322361, 0.6343985 ]), 'amountOfRareWords2': array([531., 314., 561., 482.]), 'amountOfRareWords2Percentage': array([0.15315835, 0.13628472, 0.1591941 , 0.15100251]), 'totalAmountOfWords': array([3467., 2304., 3524., 3192.]), 'commonWords': array([[3467., 1145., 1742., 1373.],\n",
      "       [1145., 2304., 1184., 1189.],\n",
      "       [1742., 1184., 3524., 1488.],\n",
      "       [1373., 1189., 1488., 3192.]])}\n"
     ]
    }
   ],
   "source": [
    "props = createPropertiesOfTrainingSet(1,50,1)\n",
    "print(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the method for creating properties...\n",
      "Starting the gathering of texts from 1 to 50...\n",
      "Gathering finished in 0.15046310424804688 ms\n",
      "Starting the preprocess...\n"
     ]
    }
   ],
   "source": [
    "props = createPropertiesOfTrainingSet(1,50,1,1)\n",
    "print(props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
