{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "2018-11-08 22:03\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import datetime\n",
    "from os import listdir\n",
    "\n",
    "class Regs: \n",
    "    specialChars = '' \n",
    "    digits = '' \n",
    "    singleChars = ''\n",
    "    multipleWhiteSpaces = ''\n",
    "    stopWords = list()\n",
    "stopWords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\"ers\", \"yourself\", \"yourselves\", \"he\",\"isnt\",\"cant\" \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"wasnt\", \"were\", \"be\", \"been\", \"being\", \"have\", \"havent\", \"has\", \"had\", \"having\", \"do\", \"does\", \"doesnt\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "#Precompile regexes for better performance\n",
    "def compileRegexes():\n",
    "    regexes = Regs()\n",
    "    regexes.specialChars = re.compile('[^\\w\\s]')\n",
    "    regexes.digits = re.compile('\\d')\n",
    "    regexes.singleChars = re.compile('\\s.\\s')\n",
    "    regexes.multipleWhiteSpaces = re.compile('[ ]{2,}')\n",
    "    for sw in stopWords:\n",
    "        exp = '\\\\b' + sw + '+\\W'\n",
    "        regexes.stopWords.append(re.compile(exp))\n",
    "    return(regexes)\n",
    "\n",
    "#Format the index into string with length 7\n",
    "def formatIndex(index):\n",
    "    i = str(index)\n",
    "    while len(i) < 7:\n",
    "        i = \"0\" + i\n",
    "    return(i)\n",
    "\n",
    "#Gather the texts from the source, based on starting index, amount of article and class\n",
    "#=> returns array of texts and array of classes\n",
    "def gatherTexts(startingIndex, amountOfArticles, includeClasses):\n",
    "    if len(includeClasses) < 1:\n",
    "        print(\"There must be at least 1 class. eg: [0,1,2]\")\n",
    "    \n",
    "    #Create two arrays, which will be returned from this function\n",
    "    texts = []\n",
    "    types = []\n",
    "    \n",
    "    #Get data for class 0 => in our project it is FINANCE\n",
    "    if 0 in includeClasses:\n",
    "        articles = listdir(\"DATA/Finance\")\n",
    "        nArticles = len(articles)\n",
    "        #check if there are enough articles for the input paramteres\n",
    "        if (nArticles > startingIndex + amountOfArticles) :\n",
    "            for ind in range(startingIndex, startingIndex + amountOfArticles):\n",
    "                with open(\"DATA/Finance/news_\" + formatIndex(ind) + \".json\", encoding=\"utf8\") as json_data:\n",
    "                    texts.append(json.load(json_data)[\"text\"])\n",
    "                    types.append(\"0\")   \n",
    "\n",
    "    #Get data for class 1 => in our project it is SPORT\n",
    "    if 0 in includeClasses:\n",
    "        articles = listdir(\"DATA/Sports\")\n",
    "        nArticles = len(articles)\n",
    "        #check if there are enough articles for the input paramteres\n",
    "        if (nArticles > startingIndex + amountOfArticles) :\n",
    "            for ind in range(startingIndex, startingIndex + amountOfArticles):\n",
    "                with open(\"DATA/Sports/news_\" + formatIndex(ind) + \".json\", encoding=\"utf8\") as json_data:\n",
    "                    texts.append(json.load(json_data)[\"text\"])\n",
    "                    types.append(\"1\")   \n",
    "                    \n",
    "    #Get data for class 2 => in our project it is TECHNOLOGY\n",
    "    if 0 in includeClasses:\n",
    "        articles = listdir(\"DATA/Technology\")\n",
    "        nArticles = len(articles)\n",
    "        #check if there are enough articles for the input paramteres\n",
    "        if (nArticles > startingIndex + amountOfArticles) :\n",
    "            for ind in range(startingIndex, startingIndex + amountOfArticles):\n",
    "                with open(\"DATA/Technology/news_\" + formatIndex(ind) + \".json\", encoding=\"utf8\") as json_data:\n",
    "                    texts.append(json.load(json_data)[\"text\"])\n",
    "                    types.append(\"2\")   \n",
    "                    \n",
    "    #Get data for class 3 => in our project it is ENTERTAINMENT\n",
    "    if 0 in includeClasses:\n",
    "        articles = listdir(\"DATA/Entertainment\")\n",
    "        nArticles = len(articles)\n",
    "        #check if there are enough articles for the input paramteres\n",
    "        if (nArticles > startingIndex + amountOfArticles) :\n",
    "            for ind in range(startingIndex, startingIndex + amountOfArticles):\n",
    "                with open(\"DATA/Entertainment/news_\" + formatIndex(ind) + \".json\", encoding=\"utf8\") as json_data:\n",
    "                    texts.append(json.load(json_data)[\"text\"])\n",
    "                    types.append(\"3\") \n",
    "                    \n",
    "    return {'texts':texts, 'types':types}\n",
    "\n",
    "def preprocessTexts(texts,types, regexes, usePorterStemmer = 0, removeUnfrequent = 0, removeFrequent = 0):    \n",
    "    articles = []\n",
    "    for index, text in enumerate(texts, start=0):\n",
    "        articles.append(preprocessArticle(text,regexes,usePorterStemmer))\n",
    "        \n",
    "    #create one array fromm all articles\n",
    "    words = [item for sublist in articles for item in sublist]\n",
    "    #remove duplicate values from words list\n",
    "    words = list(set(words))\n",
    "    words = sorted(words)\n",
    "    wordsDictionary = dict((v, i) for i, v in enumerate(words))\n",
    "    \n",
    "    articleWords = np.zeros((len(articles), len(words) + 1))\n",
    "    for index, article in enumerate(articles, start=0):\n",
    "        articleWords[index, 0 ] = types[index] \n",
    "        for j, word in enumerate(article, start=0):\n",
    "            if word != '':\n",
    "                articleWords[index, wordsDictionary[word] + 1] = 1   \n",
    "                \n",
    "    #Remove words with occurance = removeUnfrequent\n",
    "    if removeUnfrequent >= 1:    \n",
    "        indexes = []\n",
    "        for word in wordsDictionary:\n",
    "            if(sum(articleWords[:,wordsDictionary[word]]) > removeUnfrequent):\n",
    "                indexes.append(wordsDictionary[word])\n",
    "        ind = np.array(indexes)\n",
    "        words = np.array(words)\n",
    "        words = words[ind].tolist()\n",
    "        articleWords = articleWords[:,np.insert(ind + 1, 0,0, axis=0)]\n",
    "    \n",
    "    #Remove words, which are the most frequent => TOP removeFrequent will be removed\n",
    "    if removeFrequent > 0:\n",
    "        usage = np.zeros(len(words)+1)\n",
    "        for i in range(1, len(words)+1):\n",
    "            usage[i] = sum(articleWords[:,i])\n",
    "        for i in range(0, removeFrequent):\n",
    "            words = np.delete(words,usage.tolist().index(max(usage)),0)\n",
    "            articleWords = np.delete(articleWords, usage.tolist().index(max(usage)),1)\n",
    "            usage = np.delete(usage,usage.tolist().index(max(usage)),0)\n",
    "            \n",
    "    return{'articleWords':articleWords, 'words':words}\n",
    "\n",
    "\n",
    "#Create matrix containg all Phi values\n",
    "#returns array(nClasses x amountOfWords)\n",
    "def createPhiMatrix(articleWords, includeClasses):\n",
    "    numberOfClasses = len(includeClasses)\n",
    "    Y = np.zeros((numberOfClasses, articleWords.shape[1]))\n",
    "    \n",
    "    sumClasses = np.zeros((numberOfClasses))\n",
    "    #number of rows clasified to first / second group\n",
    "    for c, index in enumerate(includeClasses, start = 0):\n",
    "        sumClasses[index] = float(len(articleWords[articleWords[:,0] == c,0]))\n",
    "\n",
    "    #Laplace => alfa = 1\n",
    "    alfa = 1\n",
    "\n",
    "    #calcualting Phi_{y=0} and Phi_{y=1}\n",
    "    for j in range(0, numberOfClasses):\n",
    "        Y[j,0] = (sumClasses[j] + alfa) / float(articleWords.shape[0] + numberOfClasses * alfa)\n",
    "\n",
    "    for j in range(1, articleWords.shape[1]):\n",
    "        #calcualting Phi_{j|y=0} and Phi_{j|y=1}\n",
    "        for k in range(0, numberOfClasses):\n",
    "            Y[k,j] = (np.sum(articleWords[articleWords[:,0]==includeClasses[k],j]) + alfa) / (sumClasses[k] + numberOfClasses * alfa)\n",
    "    \n",
    "    return(Y)\n",
    "    \n",
    "#Preprocess the article with the precompiled regexes, and optional stemmer \n",
    "#=> returns the text splitted into array of words\n",
    "def preprocessArticle(text, regexes, usePorterStemmer):\n",
    "    text = text.lower()\n",
    "    #new lines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('_', ' ')\n",
    "    #special characters\n",
    "    text = re.sub(regexes.specialChars, ' ', text)\n",
    "    #digits\n",
    "    text = re.sub(regexes.digits, '', text)\n",
    "    #stopwords\n",
    "    for sw in regexes.stopWords:\n",
    "        text = re.sub(sw , '', text)\n",
    "    #single characters (ex donald j trump => donald trump)\n",
    "    text = re.sub(regexes.singleChars, ' ', text)\n",
    "    #multiple white spaces\n",
    "    text = re.sub(regexes.multipleWhiteSpaces, ' ', text)\n",
    "    if usePorterStemmer == 1:\n",
    "        splitted = text.split()\n",
    "        for index, word in enumerate(splitted, start=0):\n",
    "            splitted[index] = ps.stem(word)\n",
    "        return splitted\n",
    "    return(text.split())\n",
    "\n",
    "\n",
    "#Make prediction based on \n",
    "def predict(phiMatrix, text,words, regexes, usePorterStemmer):\n",
    "    nClasses = phiMatrix.shape[0]\n",
    "    Y = phiMatrix\n",
    "    textPorcessed = preprocessArticle(text,regexes,usePorterStemmer)\n",
    "    testArticleWords = np.zeros((1, len(words)))\n",
    "    for k, word in enumerate(textPorcessed, start=0):\n",
    "            if word in words:\n",
    "                testArticleWords[0, words.index(word)] = 1\n",
    "\n",
    "    P = np.zeros(nClasses)\n",
    "    for j in range(0, nClasses):\n",
    "        P[j] = np.log(Y[j,0]) + ((np.log(np.array(Y[j,1:]*testArticleWords)[testArticleWords == 1]))).sum() + ((np.log(np.array((1 - Y[j,1:])*(testArticleWords - 1)*-1)[testArticleWords == 0]))).sum()\n",
    "\n",
    "    classPossibilities = np.zeros(4)\n",
    "    for i in range(0,phiMatrix.shape[0]):\n",
    "        classPossibilities[i] = 1 / (sum(np.exp(P-P[i])))\n",
    "    \n",
    "    return(np.argmax(classPossibilities))\n",
    "\n",
    "print(\"Done\")\n",
    "print(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00005\n"
     ]
    }
   ],
   "source": [
    "ind = str(5)\n",
    "\n",
    "while len(ind) < 5:\n",
    "    ind = \"0\" + ind\n",
    "    \n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    " for i in range(1, 5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "result =  gatherTexts(1,10,np.array([0,1,2,3]))\n",
    "regs = compileRegexes()\n",
    "r2 = preprocessTexts(result[\"texts\"],result[\"types\"],regs)\n",
    "phi = createPhiMatrix(r2[\"articleWords\"],np.array([0,1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373.4642857142895\n"
     ]
    }
   ],
   "source": [
    "print(sum(phi[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
